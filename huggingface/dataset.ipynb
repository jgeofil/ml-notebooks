{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /home/jeremygf/git/ml-notebooks/.venv/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jeremygf/git/ml-notebooks/.venv/lib/python3.11/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jeremygf/git/ml-notebooks/.venv/lib/python3.11/site-packages (from requests) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jeremygf/git/ml-notebooks/.venv/lib/python3.11/site-packages (from requests) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jeremygf/git/ml-notebooks/.venv/lib/python3.11/site-packages (from requests) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://api.github.com/repos/huggingface/datasets/issues/6551',\n",
       "  'repository_url': 'https://api.github.com/repos/huggingface/datasets',\n",
       "  'labels_url': 'https://api.github.com/repos/huggingface/datasets/issues/6551/labels{/name}',\n",
       "  'comments_url': 'https://api.github.com/repos/huggingface/datasets/issues/6551/comments',\n",
       "  'events_url': 'https://api.github.com/repos/huggingface/datasets/issues/6551/events',\n",
       "  'html_url': 'https://github.com/huggingface/datasets/pull/6551',\n",
       "  'id': 2062768400,\n",
       "  'node_id': 'PR_kwDODunzps5jEi1C',\n",
       "  'number': 6551,\n",
       "  'title': 'Fix parallel downloads for datasets without scripts',\n",
       "  'user': {'login': 'lhoestq',\n",
       "   'id': 42851186,\n",
       "   'node_id': 'MDQ6VXNlcjQyODUxMTg2',\n",
       "   'avatar_url': 'https://avatars.githubusercontent.com/u/42851186?v=4',\n",
       "   'gravatar_id': '',\n",
       "   'url': 'https://api.github.com/users/lhoestq',\n",
       "   'html_url': 'https://github.com/lhoestq',\n",
       "   'followers_url': 'https://api.github.com/users/lhoestq/followers',\n",
       "   'following_url': 'https://api.github.com/users/lhoestq/following{/other_user}',\n",
       "   'gists_url': 'https://api.github.com/users/lhoestq/gists{/gist_id}',\n",
       "   'starred_url': 'https://api.github.com/users/lhoestq/starred{/owner}{/repo}',\n",
       "   'subscriptions_url': 'https://api.github.com/users/lhoestq/subscriptions',\n",
       "   'organizations_url': 'https://api.github.com/users/lhoestq/orgs',\n",
       "   'repos_url': 'https://api.github.com/users/lhoestq/repos',\n",
       "   'events_url': 'https://api.github.com/users/lhoestq/events{/privacy}',\n",
       "   'received_events_url': 'https://api.github.com/users/lhoestq/received_events',\n",
       "   'type': 'User',\n",
       "   'site_admin': False},\n",
       "  'labels': [],\n",
       "  'state': 'open',\n",
       "  'locked': False,\n",
       "  'assignee': None,\n",
       "  'assignees': [],\n",
       "  'milestone': None,\n",
       "  'comments': 1,\n",
       "  'created_at': '2024-01-02T18:06:18Z',\n",
       "  'updated_at': '2024-01-02T18:20:40Z',\n",
       "  'closed_at': None,\n",
       "  'author_association': 'MEMBER',\n",
       "  'active_lock_reason': None,\n",
       "  'draft': False,\n",
       "  'pull_request': {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/6551',\n",
       "   'html_url': 'https://github.com/huggingface/datasets/pull/6551',\n",
       "   'diff_url': 'https://github.com/huggingface/datasets/pull/6551.diff',\n",
       "   'patch_url': 'https://github.com/huggingface/datasets/pull/6551.patch',\n",
       "   'merged_at': None},\n",
       "  'body': 'Enable parallel downloads using multiprocessing when `num_proc` is passed to `load_dataset`.\\r\\n\\r\\nIt was enabled for datasets with scripts already (if they passed lists to `dl_manager.download`) but not for no-script datasets (we pass dicts {split: [list of files]} to `dl_manager.download` for those ones).\\r\\n\\r\\nI fixed this by parallelising on the lists contained in the data files dicts when possible.\\r\\n\\r\\nI also added a context manager `stack_multiprocessing_download_progress_bars` in `DownloadManager` to stack the progress bard of the downloads (from `cached_path(...)` calls). Otherwise the progress bars overlap each other with an annoying flickering effect.',\n",
       "  'reactions': {'url': 'https://api.github.com/repos/huggingface/datasets/issues/6551/reactions',\n",
       "   'total_count': 0,\n",
       "   '+1': 0,\n",
       "   '-1': 0,\n",
       "   'laugh': 0,\n",
       "   'hooray': 0,\n",
       "   'confused': 0,\n",
       "   'heart': 0,\n",
       "   'rocket': 0,\n",
       "   'eyes': 0},\n",
       "  'timeline_url': 'https://api.github.com/repos/huggingface/datasets/issues/6551/timeline',\n",
       "  'performed_via_github_app': None,\n",
       "  'state_reason': None}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "\n",
    "url = \"https://api.github.com/repos/huggingface/datasets/issues?page=1&per_page=1\"\n",
    "response = requests.get(url)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['GITHUB_TOKEN'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import dotenv_values\n",
    "\n",
    "config = dotenv_values()\n",
    "config.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "GITHUB_TOKEN = config[\"GITHUB_TOKEN\"]\n",
    "headers = {\"Authorization\": f\"token {GITHUB_TOKEN}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def fetch_issues(\n",
    "    owner=\"huggingface\",\n",
    "    repo=\"datasets\",\n",
    "    num_issues=10_000,\n",
    "    rate_limit=5_000,\n",
    "    issues_path=Path(\"output_dir/git_issues\"),\n",
    "):\n",
    "    if not issues_path.is_dir():\n",
    "        issues_path.mkdir(exist_ok=True)\n",
    "\n",
    "    batch = []\n",
    "    all_issues = []\n",
    "    per_page = 100  # Number of issues to return per page\n",
    "    num_pages = math.ceil(num_issues / per_page)\n",
    "    base_url = \"https://api.github.com/repos\"\n",
    "\n",
    "    for page in tqdm(range(num_pages)):\n",
    "        # Query with state=all to get both open and closed issues\n",
    "        query = f\"issues?page={page}&per_page={per_page}&state=all\"\n",
    "        issues = requests.get(f\"{base_url}/{owner}/{repo}/{query}\", headers=headers)\n",
    "        batch.extend(issues.json())\n",
    "\n",
    "        if len(batch) > rate_limit and len(all_issues) < num_issues:\n",
    "            all_issues.extend(batch)\n",
    "            batch = []  # Flush batch for next time period\n",
    "            print(f\"Reached GitHub rate limit. Sleeping for one hour ...\")\n",
    "            time.sleep(60 * 60 + 1)\n",
    "\n",
    "    all_issues.extend(batch)\n",
    "    df = pd.DataFrame.from_records(all_issues)\n",
    "    df.to_json(f\"{issues_path}/{repo}-issues.jsonl\", orient=\"records\", lines=True)\n",
    "    print(\n",
    "        f\"Downloaded all the issues for {repo}! Dataset stored at {issues_path}/{repo}-issues.jsonl\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc93e25a7134452bbfb06faf517379e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached GitHub rate limit. Sleeping for one hour ...\n"
     ]
    }
   ],
   "source": [
    "# Depending on your internet connection, this can take several minutes to run...\n",
    "fetch_issues()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
